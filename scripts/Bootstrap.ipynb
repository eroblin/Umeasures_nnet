{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.util import Surv as skSurv\n",
    "from sksurv.metrics import concordance_index_ipcw, cumulative_dynamic_auc, brier_score, integrated_brier_score\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.utils import resample\n",
    "from utils.param_search import *\n",
    "from utils.output_results import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uncertainty measure used here is the Bootstrap method.\n",
    "\n",
    "2 real datasets can be used : \n",
    "- the LungCancerExplorer dataset is a set composed of multiple sources of data\n",
    "- the METABRIC cohort.\n",
    "\n",
    "We apply this method to different types of neural network models :\n",
    "- CoxCC, CoxTime\n",
    "- DeepHit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', '-dt', default='Metabric', type=str) #LungCancerExplorer, Metabric\n",
    "parser.add_argument('--name', '-n',type=str, default=\"CoxCC\")#CoxTime, DeepHit, CoxCC \n",
    "parser.add_argument('--plot_mode', '-pm', default=False, action='store_true')\n",
    "parser.add_argument('--timepoints', '-tp',type=str, default=\"fixed\") #fixed, percentiles\n",
    "parser.add_argument('--uncertainty', '-u',type=str, default=\"Bootstrap\") #Bootstrap, MCDropout, DeepEnsemble, VAE, BMask\n",
    "config = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_res = 'results/'+ config.dataset + \"/\"+config.uncertainty+\"/\"+ config.name+'/'\n",
    "os.makedirs(dir_res, exist_ok=True)\n",
    "\n",
    "dir_data = \"data/\" + config.dataset + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data are previously handled using the MICE package in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dir_data+\"DataImputed.csv\")\n",
    "df['id'] = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We list the clinical variables and the genes variables according to the dataset considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.dataset == \"Metabric\":\n",
    "    ClinVar =['age',\n",
    "                   'chemotherapy',\n",
    "                   'grade1',\n",
    "                   'grade2',\n",
    "                   'hormonotherapy',\n",
    "                   'N',\n",
    "                   'tumor_size']\n",
    "    ColsLeave = ['chemotherapy',\n",
    "                  'grade1',\n",
    "                  'grade2',\n",
    "                  'id',\n",
    "                  'hormonotherapy',\n",
    "                  'N',\n",
    "                  'status',\n",
    "                  'yy']\n",
    "elif config.dataset == \"LungCancerExplorer\":\n",
    "    ClinVar = ['Pat_Age',\n",
    "                    'Pat_Stage_II',\n",
    "                    'Pat_Stage_III',\n",
    "                    'Pat_Stage_IV',\n",
    "                    'Pat_Stage_I_or_II']\n",
    "    ColsLeave = ['id',\n",
    "                  'Pat_Stage_II',\n",
    "                  'Pat_Stage_III',\n",
    "                  'Pat_Stage_IV',\n",
    "                  'Pat_Stage_I_or_II',\n",
    "                  'status',\n",
    "                  'yy']\n",
    "GenesVar = df.drop(columns = ClinVar + ['id','status','yy'],axis=1).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 folds of the simple cross-validation are defined and saved beforehand in order to always have the same folds between the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_data+\"folds_1CV.json\") as f:\n",
    "    kfolds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[ClinVar+GenesVar+['yy','status','id']].loc[kfolds['train']] \n",
    "df_test = df[ClinVar+GenesVar+['yy','status','id']].loc[kfolds['test']] \n",
    "AllVar = ClinVar+GenesVar\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous variable are standardized. The yy variable corresponds to the survival time, the status is the censoring indicator (a value of 0 corresponds to censoring) and the id variable is the id of the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ColsStandardize = [col for col in AllVar if col not in ColsLeave] \n",
    "standardize = [([col], StandardScaler()) for col in ColsStandardize]\n",
    "leave = [(col, None) for col in ColsLeave]\n",
    "df_mapper = DataFrameMapper(standardize + leave, df_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparmater Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple 5-folds cross validation is implemented using the training set to determine the hyperparameters of the neural network models. The optuna package is used to perform the hyperparmeter search. The hyperparameter that are searched are the following:\n",
    "\n",
    "| Hyperparameter | Values |\n",
    "|----------|--------------|\n",
    "| Activation function |  {tanh, relu} |\n",
    "| Batch size |  {8,16,32,64,128} |\n",
    "| Dropout rate |  [0.0,0.3] | \n",
    "|Layers | {1,2,3,4}|\n",
    "|Learning rate|[1e-3, 1e-2]|\n",
    "|Neurons|[4,128]|\n",
    "|Optimizer|{adam, adam_amsgrad, RMSProp, SGDWR}|\n",
    "|PÃ©nalisation L2|[0,0.1]|\n",
    "|Alpha (DeepHit)|[0,1]|\n",
    "|Sigma(DeepHit)|{0.1,0.25,0.5,1,2.5,5,10,100}|\n",
    "|Durations(DeepHit)|{10,50,100,200,400}|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(study_name = config.name, \n",
    "                            storage = 'sqlite:///'+dir_res+config.name+'.db',\n",
    "                            sampler=sampler, \n",
    "                            direction='minimize', \n",
    "                            load_if_exists=True)\n",
    "\n",
    "\n",
    "study.optimize(lambda trial : objective_net(trial,\n",
    "                                            df_train,\n",
    "                                            df_mapper,\n",
    "                                            dir_res,\n",
    "                                            config), \n",
    "                                           n_trials=200)\n",
    "trial = study.best_trial\n",
    "outer_loop = pd.DataFrame([trial.params])\n",
    "outer_loop.to_csv(dir_res+'best_param.csv', sep = ';', index = False, header = True)\n",
    "df_results = study.trials_dataframe()\n",
    "df_results.to_csv(dir_res+'trials_dataframe.csv', sep = ';', header = True)\n",
    "\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.write_image(dir_res+\"optim_history.png\")\n",
    "fig2 = optuna.visualization.plot_slice(study)\n",
    "fig2.write_image(dir_res+\"parameters_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Measure using DeepEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model using always the same hyperparameters that were selected by cross validaton. We sample with replacement data from the training set.  Then, the model is trained on this  bootstraped dataset. We repeat the sampling M times, obtaining M predictions per point of the test set. This is the method called [Bootstrap](https://www.jstor.org/stable/2958830)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the hyperparameters selected previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_loop = pd.read_csv(dir_res+'best_param.csv', sep = ';')\n",
    "config.acti_func = outer_loop['activation'][0]\n",
    "config.batch_size = outer_loop['batch_size'][0]\n",
    "config.dr = outer_loop['dropout'][0]\n",
    "config.layers = outer_loop['n_layers'][0]\n",
    "config.lr  = outer_loop['learning_rate'][0]\n",
    "config.neurons = outer_loop['neurons'][0]\n",
    "config.optim = outer_loop['optimizer'][0]\n",
    "config.pen_l2 = outer_loop['l2'][0]\n",
    "\n",
    "if config.name==\"DeepHit\":\n",
    "    config.alpha = outer_loop['alpha'][0] \n",
    "    config.sigma = outer_loop['sigma'][0]\n",
    "    config.num_durations = outer_loop['num_durations'][0]\n",
    "    labtrans = DeepHitSingle.label_transform(config.num_durations)\n",
    "elif config.name==\"CoxTime\":\n",
    "    labtrans = CoxTime.label_transform()\n",
    "else:\n",
    "    labtrans=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the timepoints are defined as percentiles, the results are outputed at the percentiles of survival times of the simulation data. If the timepoints are deifned as fixed, teh results are outputed at the same specific times for all the simulation datasets.\n",
    "\n",
    "For the breast cancer dataset, the fixed timepoints used are at 5 and 10 years. For the LungCancerExplorer, we output the measures at 2 and 5 years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.timepoints == \"percentiles\":\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(np.array(df_train['yy']), np.array(df_train['status']))\n",
    "    time_grid = np.array(kmf.percentile(np.linspace(0.9, 0.1, 9)).iloc[:,0])\n",
    "elif config.timepoints == \"fixed\":\n",
    "    if config.dataset == \"Metabric\":\n",
    "        time_grid = [2,5]\n",
    "    elif config.dataset == \"LungCancerExplorer\":\n",
    "        time_grid = [24,60]\n",
    "    else:\n",
    "        time_grid = [0.5,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the number of M we sample with replacement the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the Concordance Index (CAll), the Brier Score (BSAll) and the survival predictions for all the patients of the test set (PredAll) for each timepoint of the time grid previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAll = pd.DataFrame()\n",
    "BSAll = pd.DataFrame()\n",
    "PredAll = []\n",
    "measures = pd.DataFrame()\n",
    "\n",
    "for j in range(M):\n",
    "    print(j)\n",
    "    #Bootstrap of the train set and split into validation and train set\n",
    "    df_trainb = df_train.iloc[resample(df_train.index, replace=True, n_samples=len(df_train),random_state=j)]\n",
    "    df_valb = df_trainb.sample(frac=0.2, random_state = j)\n",
    "    df_trainb = df_trainb.drop(df_valb.index)\n",
    "\n",
    "    df_trainb = df_mapper.fit_transform(df_trainb)\n",
    "    df_valb = df_mapper.transform(df_valb).astype('float32')\n",
    "    df_test = df_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "    x_train = np.array(df_trainb.drop(['yy','status','id'], axis=1)).astype('float32')\n",
    "    x_val = np.array(df_valb.drop(['yy','status','id'], axis=1)).astype('float32')\n",
    "    x_test = np.array(df_test.drop(['yy','status','id'], axis=1)).astype('float32')\n",
    "    y_train = (df_trainb['yy'].values, df_trainb['status'].values)\n",
    "    y_val = (df_valb['yy'].values, df_valb['status'].values)\n",
    "    y_test = (df_test['yy'].values, df_test['status'].values)\n",
    "\n",
    "    if labtrans !=\"\":\n",
    "        y_train = labtrans.fit_transform(*y_train)\n",
    "        y_val = labtrans.transform(*y_val)\n",
    "\n",
    "    val = tt.tuplefy(x_val, y_val)\n",
    "\n",
    "    in_features = x_train.shape[1]\n",
    "    model,callbacks = build_model_net(config,in_features,labtrans)\n",
    "\n",
    "    log = model.fit(x_train, \n",
    "                y_train, \n",
    "                int(config.batch_size),\n",
    "                epochs = 500, \n",
    "                callbacks = callbacks,\n",
    "                verbose = False,\n",
    "                val_data = val,\n",
    "                shuffle=True)\n",
    "    \n",
    "    #Output of the survival probabilities\n",
    "    if config.name in [\"CoxCC\",\"CoxTime\"]:\n",
    "        _ = model.compute_baseline_hazards()\n",
    "        surv = model.predict_surv_df(x_test)\n",
    "    elif config.name == \"DeepHit\":\n",
    "         surv = model.interpolate(10).predict_surv_df(x_test)\n",
    "\n",
    "    #Output of the evaluation measures on the test set at predifined time points\n",
    "    data_train = skSurv.from_arrays(event=df_train['status'], time=df_train['yy'])\n",
    "    data_test = skSurv.from_arrays(event=df_test['status'], time=df_test['yy'])\n",
    "    CAll[j] = [concordance_index_ipcw(data_train, data_test, np.array(-determine_surv_prob(surv,t)),t)[0] for t in time_grid]\n",
    "    BSAll[j] = [brier_score(data_train, data_test, np.array(-determine_surv_prob(surv,t)),t)[1][0] for t in time_grid]\n",
    "    Pred = np.asarray([determine_surv_prob(surv,t) for t in time_grid])\n",
    "    PredAll.append(Pred)\n",
    "    del model \n",
    "    del log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures['C'] = CAll.mean(axis=1)\n",
    "measures['BS'] = BSAll.mean(axis=1)\n",
    "measures['Time'] = time_grid\n",
    "measures.to_csv(dir_res+'measures_'+config.name+'.csv', sep = ';', header = True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We output survival prediction intervals for a certain number of patients (n_pat) for each timepoint of the timegrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4893)\n",
    "patients_test = random.sample(list(df_test['id']),n_pat)\n",
    "ResPat = []\n",
    "for t in range (len(time_grid)):\n",
    "    ResTime = pd.DataFrame([PredAll[l][t] for l in range(M)]).T\n",
    "    for i in patients_test:\n",
    "        ResTime.index = df_test['id']\n",
    "        ic = output_ic(ResTime.loc[i,:],0.95)\n",
    "        values = df_test.loc[:,['status','yy','id']][df_test['id']==i]\n",
    "        ResPat.append(values.values.tolist()[0]+list(ic))\n",
    "ResPat = pd.DataFrame(ResPat)\n",
    "ResPat.columns = ['status','yy','id','IClow','ICmean','IChigh']\n",
    "ResPat[\"Time\"] = np.repeat(time_grid,n_pat)\n",
    "ResPat.to_csv(dir_res+\"surv_intervals.csv\", sep=';', header = True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResPat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

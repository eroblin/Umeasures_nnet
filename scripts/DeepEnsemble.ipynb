{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.util import Surv as skSurv\n",
    "from sksurv.metrics import concordance_index_ipcw, cumulative_dynamic_auc, brier_score, integrated_brier_score\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.utils import resample\n",
    "from utils.param_search import *\n",
    "from utils.output_results import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3700163-2260-424f-9b83-05f5270feeb2",
   "metadata": {},
   "source": [
    "The uncertainty measure used here is the Deep Ensemble method.\n",
    "\n",
    "2 real datasets can be used : \n",
    "- the LungCancerExplorer dataset is a set composed of multiple sources of data\n",
    "- the METABRIC cohort.\n",
    "\n",
    "We apply this method to different types of neural network models :\n",
    "- CoxCC, CoxTime\n",
    "- DeepHit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4612003",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', '-dt', default='LungCancerExplorer', type=str) #LungCancerExplorer, Metabric\n",
    "parser.add_argument('--name', '-n',type=str, default=\"CoxCC\")#CoxTime, DeepHit, CoxCC \n",
    "parser.add_argument('--plot_mode', '-pm', default=False, action='store_true')\n",
    "parser.add_argument('--timepoints', '-tp',type=str, default=\"fixed\") #fixed, percentiles\n",
    "parser.add_argument('--uncertainty', '-u',type=str, default=\"DeepEnsemble\") #Bootstrap, MCDropout, DeepEnsemble, VAE, BMask\n",
    "config = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc736f2-4aa3-4584-970b-b71e1674743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_res = 'results/'+ config.dataset + \"/\"+config.uncertainty+\"/\"+ config.name+'/'\n",
    "os.makedirs(dir_res, exist_ok=True)\n",
    "\n",
    "dir_data = \"data/\" + config.dataset + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a53a4",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8acca5-f88e-492f-9df6-f6c2a399c930",
   "metadata": {},
   "source": [
    "Missing data are previously handled using the MICE package in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af645f01-3b65-477f-b169-1d7f1ca84324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dir_data+\"DataImputed.csv\")\n",
    "df['id'] = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea8843-ae36-4deb-9936-f29e64162f04",
   "metadata": {},
   "source": [
    "We list the clinical variables and the genes variables according to the dataset considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ffb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.dataset == \"Metabric\":\n",
    "    ClinVar =['age',\n",
    "                   'chemotherapy',\n",
    "                   'grade1',\n",
    "                   'grade2',\n",
    "                   'hormonotherapy',\n",
    "                   'N',\n",
    "                   'tumor_size']\n",
    "    ColsLeave = ['chemotherapy',\n",
    "                  'grade1',\n",
    "                  'grade2',\n",
    "                  'id',\n",
    "                  'hormonotherapy',\n",
    "                  'N',\n",
    "                  'status',\n",
    "                  'yy']\n",
    "elif config.dataset == \"LungCancerExplorer\":\n",
    "    ClinVar = ['Pat_Age',\n",
    "                    'Pat_Stage_II',\n",
    "                    'Pat_Stage_III',\n",
    "                    'Pat_Stage_IV',\n",
    "                    'Pat_Stage_I_or_II']\n",
    "    ColsLeave = ['id',\n",
    "                  'Pat_Stage_II',\n",
    "                  'Pat_Stage_III',\n",
    "                  'Pat_Stage_IV',\n",
    "                  'Pat_Stage_I_or_II',\n",
    "                  'status',\n",
    "                  'yy']\n",
    "GenesVar = df.drop(columns = ClinVar + ['id','status','yy'],axis=1).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24cdc11-18a8-4e46-80a5-ad5b247185c9",
   "metadata": {},
   "source": [
    "The 5 folds of the simple cross-validation are defined and saved beforehand in order to always have the same folds between the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab18dd7-ef7b-4e50-a16a-0ba330da1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_data+\"folds_1CV.json\") as f:\n",
    "    kfolds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c428c1-1906-48dd-95bc-3337069aad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[ClinVar+GenesVar+['yy','status','id']].loc[kfolds['train']] \n",
    "df_test = df[ClinVar+GenesVar+['yy','status','id']].loc[kfolds['test']] \n",
    "AllVar = ClinVar+GenesVar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc028482-4009-40cb-81c5-f651c2c1db94",
   "metadata": {},
   "source": [
    "The continuous variable are standardized. The yy variable corresponds to the survival time, the status is the censoring indicator (a value of 0 corresponds to censoring) and the id variable is the id of the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d0921-dc7e-407e-9e63-1cfb34bb5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "ColsStandardize = [col for col in AllVar if col not in ColsLeave] \n",
    "standardize = [([col], StandardScaler()) for col in ColsStandardize]\n",
    "leave = [(col, None) for col in ColsLeave]\n",
    "df_mapper = DataFrameMapper(standardize + leave, df_out=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416dbd6",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd5d38",
   "metadata": {},
   "source": [
    "A simple 5-folds cross validation is implemented using the training set to determine the hyperparameters of the neural network models. The optuna package is used to perform the hyperparmeter search. The hyperparameter that are searched are the following:\n",
    "\n",
    "| Hyperparameter | Values |\n",
    "|----------|--------------|\n",
    "| Activation function |  {tanh, relu} |\n",
    "| Batch size |  {8,16,32,64,128} |\n",
    "| Dropout rate |  [0.0,0.3] | \n",
    "|Layers | {1,2,3,4}|\n",
    "|Learning rate|[1e-3, 1e-2]|\n",
    "|Neurons|[4,128]|\n",
    "|Optimizer|{adam, adam_amsgrad, RMSProp, SGDWR}|\n",
    "|PÃ©nalisation L2|[0,0.1]|\n",
    "|Alpha (DeepHit)|[0,1]|\n",
    "|Sigma(DeepHit)|{0.1,0.25,0.5,1,2.5,5,10,100}|\n",
    "|Durations(DeepHit)|{10,50,100,200,400}|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac7c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(study_name = config.name, \n",
    "                            storage = 'sqlite:///'+dir_res+config.name+'.db',\n",
    "                            sampler=sampler, \n",
    "                            direction='minimize', \n",
    "                            load_if_exists=True)\n",
    "\n",
    "\n",
    "study.optimize(lambda trial : objective_net(trial,\n",
    "                                            df_train,\n",
    "                                            df_mapper,\n",
    "                                            dir_res,\n",
    "                                            config), \n",
    "                                           n_trials=2)\n",
    "trial = study.best_trial\n",
    "outer_loop = pd.DataFrame([trial.params])\n",
    "outer_loop.to_csv(dir_res+'best_param.csv', sep = ';', index = False, header = True)\n",
    "df_results = study.trials_dataframe()\n",
    "df_results.to_csv(dir_res+'trials_dataframe.csv', sep = ';', header = True)\n",
    "\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.write_image(dir_res+\"optim_history.png\")\n",
    "fig2 = optuna.visualization.plot_slice(study)\n",
    "fig2.write_image(dir_res+\"parameters_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d00bdd",
   "metadata": {},
   "source": [
    "# Uncertainty Measure using DeepEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1fe262",
   "metadata": {},
   "source": [
    "We build the model using always the same hyperparameters that were selected by cross validaton. Then, the model is trained M times, randomly initializing the weights each time. The training set is randomly shuffled. The training criterion is the loss. M predictions per point of the test set are outputed. This is the method called [DeepEnsemble](https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b06be-62ff-4d68-88f3-3e1ee5f66ebd",
   "metadata": {},
   "source": [
    "We load the hyperparameters selected previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1311bd0-b687-424d-a8d2-cddaad73ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_loop = pd.read_csv(dir_res+'best_param.csv', sep = ';')\n",
    "config.acti_func = outer_loop['activation'][0]\n",
    "config.batch_size = outer_loop['batch_size'][0]\n",
    "config.dr = outer_loop['dropout'][0]\n",
    "config.layers = outer_loop['n_layers'][0]\n",
    "config.lr  = outer_loop['learning_rate'][0]\n",
    "config.neurons = outer_loop['neurons'][0]\n",
    "config.optim = outer_loop['optimizer'][0]\n",
    "config.pen_l2 = outer_loop['l2'][0]\n",
    "\n",
    "if config.name==\"DeepHit\":\n",
    "    config.alpha = outer_loop['alpha'][0] \n",
    "    config.sigma = outer_loop['sigma'][0]\n",
    "    config.num_durations = outer_loop['num_durations'][0]\n",
    "    labtrans = DeepHitSingle.label_transform(config.num_durations)\n",
    "elif config.name==\"CoxTime\":\n",
    "    labtrans = CoxTime.label_transform()\n",
    "else:\n",
    "    labtrans=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2555a-a253-484b-a969-c344851062df",
   "metadata": {},
   "source": [
    "We define the number of repetitions M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04834480-11a6-45f2-84ad-240eff12ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bbae4e-8b7f-433b-afb4-731492bd8475",
   "metadata": {},
   "source": [
    "We sample 20\\% of the train set and define it as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51bcaf-5311-4f4c-a571-731a2fe90ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_train.sample(frac=0.2, random_state = 1234)\n",
    "df_train = df_train.drop(df_val.index)\n",
    "df_train = df_mapper.fit_transform(df_train)\n",
    "df_val = df_mapper.transform(df_val).astype('float32')\n",
    "df_test = df_mapper.transform(df_test).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df603655-130a-41df-85f2-9b60d9e83a8f",
   "metadata": {},
   "source": [
    "If the timepoints are defined as percentiles, the results are outputed at the percentiles of survival times. If the timepoints are deifned as fixed, teh results are outputed at specific times.\n",
    "\n",
    "For the breast cancer dataset, the fixed timepoints used are at 5 and 10 years. For the LungCancerExplorer, we output the measures at 2 and 5 years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd379389-bd63-4965-a528-8cb4c1274a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.timepoints == \"percentiles\":\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(np.array(df_train['yy']), np.array(df_train['status']))\n",
    "    time_grid = np.array(kmf.percentile(np.linspace(0.9, 0.1, 9)).iloc[:,0])\n",
    "elif config.timepoints == \"fixed\":\n",
    "    if config.dataset == \"Metabric\":\n",
    "        time_grid = [2,5]\n",
    "    elif config.dataset == \"LungCancerExplorer\":\n",
    "        time_grid = [24,60]\n",
    "    else:\n",
    "        time_grid = [0.5,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d66dc-5d68-4aab-9571-7f4a5ad46064",
   "metadata": {},
   "source": [
    "We define x and y for the train set, the validation set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce58c3-adce-40be-bd56-f1905e36dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(df_train.drop(['yy','status','id'], axis=1)).astype('float32')\n",
    "x_val = np.array(df_val.drop(['yy','status','id'], axis=1)).astype('float32')\n",
    "x_test = np.array(df_test.drop(['yy','status','id'], axis=1)).astype('float32')\n",
    "y_train = (df_train['yy'].values, df_train['status'].values)\n",
    "y_val = (df_val['yy'].values, df_val['status'].values)\n",
    "y_test = (df_test['yy'].values, df_test['status'].values)\n",
    "    \n",
    "if labtrans !=\"\":\n",
    "    y_train = labtrans.fit_transform(*y_train)\n",
    "    y_val = labtrans.transform(*y_val)\n",
    "\n",
    "val = (x_val, y_val)\n",
    "in_features = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed44946-8add-4b32-9ae5-2da801bb8f9c",
   "metadata": {},
   "source": [
    "We compute the Concordance Index (CAll), the Brier Score (BSAll) and the survival predictions for all the patients of the test set (PredAll) for each timepoint of the time grid previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2ae23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CAll = pd.DataFrame()\n",
    "BSAll = pd.DataFrame()\n",
    "PredAll = []\n",
    "measures = pd.DataFrame()\n",
    "\n",
    "for j in range(M):\n",
    "    print(j)\n",
    "    model,callbacks = build_model_net(config,\n",
    "                                  in_features,\n",
    "                                  labtrans)\n",
    "    log = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    int(config.batch_size),\n",
    "                    epochs = 500, \n",
    "                    callbacks = callbacks,\n",
    "                    verbose = False,\n",
    "                    val_data = val,\n",
    "                   shuffle = True)\n",
    "\n",
    "    if config.name in [\"CoxCC\",\"CoxTime\"]:\n",
    "        _ = model.compute_baseline_hazards()\n",
    "        surv = model.predict_surv_df(x_test)\n",
    "    elif config.name == \"DeepHit\":\n",
    "         surv = model.interpolate(10).predict_surv_df(x_test)\n",
    "\n",
    "    data_train = skSurv.from_arrays(event=df_train['status'], time=df_train['yy'])\n",
    "    data_test = skSurv.from_arrays(event=df_test['status'], time=df_test['yy'])\n",
    "    CAll[j] = [concordance_index_ipcw(data_train, data_test, np.array(-determine_surv_prob(surv,t)),t)[0] for t in time_grid]\n",
    "    BSAll[j] = [brier_score(data_train, data_test, np.array(-determine_surv_prob(surv,t)),t)[1][0] for t in time_grid]\n",
    "    preds = np.asarray([determine_surv_prob(surv,t) for t in time_grid])\n",
    "    PredAll.append(preds)  \n",
    "    del model \n",
    "    del log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e34ccc-8859-4c92-8eb9-fe49b763a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures['C'] = CAll.mean(axis=1)\n",
    "measures['BS'] = BSAll.mean(axis=1)\n",
    "measures['Time'] = time_grid\n",
    "measures.to_csv(dir_res+'measures_'+config.name+'.csv', sep = ';', header = True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274dc56-a3a5-4e5e-9bd8-3a4ad37a4e97",
   "metadata": {},
   "source": [
    "We output survival prediction intervals for a certain number of patients (n_pat) for each timepoint of the timegrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46708e20-3ea2-4de7-89d7-ebe2847218e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550f32d-6fa6-44f1-8166-b46a077391af",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4893)\n",
    "patients_test = random.sample(list(df_test['id']),n_pat)\n",
    "ResPat = []\n",
    "for t in range (len(time_grid)):\n",
    "    ResTime = pd.DataFrame([PredAll[l][t] for l in range(M)]).T\n",
    "    for i in patients_test:\n",
    "        ResTime.index = df_test['id']\n",
    "        ic = output_ic(ResTime.loc[i,:],0.95)\n",
    "        values = df_test.loc[:,['status','yy','id']][df_test['id']==i]\n",
    "        ResPat.append(values.values.tolist()[0]+list(ic))\n",
    "ResPat = pd.DataFrame(ResPat)\n",
    "ResPat.columns = ['status','yy','id','IClow','ICmean','IChigh']\n",
    "ResPat[\"Time\"] = np.repeat(time_grid,n_pat)\n",
    "ResPat.to_csv(dir_res+\"surv_intervals.csv\", sep=';', header = True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa43a8-f1a2-4b65-a74c-89c904794383",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResPat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
